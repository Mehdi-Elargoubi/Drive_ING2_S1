{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (5pts) Un premier modèle\n",
        "\n",
        "On considère le jeu de données MNIST ci-dessous (les images peuvent être téléchargées via scikit learn ou via keras). On souhaite commencer par entraîner un modèle de régression logistique permettant de différencier les 1 des 0. Pour ce faire on procédera comme suit:\n",
        "\n",
        "- Extraire à partir des données ci-dessous, les images représentant des $1$ ou des $0$.\n",
        "- Séparer les données en un ensemble de test et un ensemble d'entraînement (on gardera 10% des données pour l'ensemble test)\n",
        "- Compléter la fonction \"binary_cross_entropy\" afin que celle-ci retourne la valeur de l'entropie binaire croisée ainsi que le gradient de cette fonction en une image donnée et pour un vecteur de coefficients de régression $\\mathbf{w}$ donne.\n",
        "- Compléter ensuite la fonction “optimisation\" afin qu'elle implémente une descente de gradient sur la fonction d'entropie binaire croisée. On souhaite  renvoyer en sortie le vecteur des coefficients de régression ainsi que (1) le taux de classification (en pourcentage de données correctement classées sur le nombre de données totales) sur les ensembles d'entraînement et de test.  \n",
        "\n"
      ],
      "metadata": {
        "id": "8JHT9FDu19yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "metadata": {
        "id": "nJWvQhBO3_tv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qApxpHOZ1wLB",
        "outputId": "53acc37a-48f2-4541-c28f-81dc6fe53c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 0.6931\n",
            "Epoch 10: Loss = 0.1141\n",
            "Epoch 20: Loss = 0.0688\n",
            "Epoch 30: Loss = 0.0515\n",
            "Epoch 40: Loss = 0.0421\n",
            "Epoch 50: Loss = 0.0361\n",
            "Epoch 60: Loss = 0.0320\n",
            "Epoch 70: Loss = 0.0289\n",
            "Epoch 80: Loss = 0.0265\n",
            "Epoch 90: Loss = 0.0246\n",
            "Taux de classification sur l'ensemble d'entraînement : 99.72%\n",
            "Taux de classification sur l'ensemble de test : 99.92%\n"
          ]
        }
      ],
      "source": [
        "# Charger les données MNIST\n",
        "(X, y), _ = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Aplatir les images en vecteurs de taille (28 * 28 = 784)\n",
        "X = X.reshape(X.shape[0], -1)  # Chaque image devient un vecteur de 784 éléments\n",
        "X = X / 255.0  # Normaliser les images entre 0 et 1\n",
        "\n",
        "# Filtrer les étiquettes pour ne garder que les chiffres '0' et '1'\n",
        "mask = (y == 0) | (y == 1)\n",
        "X, y = X[mask], y[mask]\n",
        "\n",
        "# Fonction sigmoïde\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Fonction binaire croisée et son gradient\n",
        "def binary_cross_entropy(X, y, w):\n",
        "    m = X.shape[0]\n",
        "    z = np.dot(X, w)\n",
        "    predictions = sigmoid(z)\n",
        "\n",
        "    # Calcul de l'entropie binaire croisée\n",
        "    bce_fun = -np.mean(y * np.log(predictions + 1e-8) + (1 - y) * np.log(1 - predictions + 1e-8))\n",
        "\n",
        "    # Calcul du gradient\n",
        "    bce_grad = np.dot(X.T, (predictions - y)) / m\n",
        "\n",
        "    return bce_fun, bce_grad\n",
        "\n",
        "# Fonction d'optimisation\n",
        "def optimisation(w_init, eta, X_train, y_train, X_test, y_test, epochs=100):\n",
        "    w = w_init\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Calcul de la perte et du gradient\n",
        "        loss, grad = binary_cross_entropy(X_train, y_train, w)\n",
        "\n",
        "        # Mise à jour des coefficients\n",
        "        w -= eta * grad\n",
        "\n",
        "        # Affichage de la perte toutes les 10 itérations\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
        "\n",
        "    # Prédiction\n",
        "    def predict(X, w):\n",
        "        return (sigmoid(np.dot(X, w)) >= 0.5).astype(int)\n",
        "\n",
        "    y_train_pred = predict(X_train, w)\n",
        "    y_test_pred = predict(X_test, w)\n",
        "\n",
        "    # Taux de classification\n",
        "    rate_bce_training = np.mean(y_train_pred == y_train) * 100\n",
        "    rate_bce_test = np.mean(y_test_pred == y_test) * 100\n",
        "\n",
        "    return w, rate_bce_training, rate_bce_test\n",
        "\n",
        "# Initialisation et test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "w_init = np.zeros(X_train.shape[1])\n",
        "eta = 0.1\n",
        "\n",
        "w_opt, rate_bce_training, rate_bce_test = optimisation(w_init, eta, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(f\"Taux de classification sur l'ensemble d'entraînement : {rate_bce_training:.2f}%\")\n",
        "print(f\"Taux de classification sur l'ensemble de test : {rate_bce_test:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (5pts) Un petit réseau convolutif\n",
        "\n",
        "On souhaite à présent ameliorer le resultat de la question 1 a l'aide d'un réseau convolutionnel. Pour ce faire, on souhaite utiliser la librairie Keras (voir https://keras.io/) et en particulier, le modele de reseau convolutionnel (https://keras.io/api/layers/convolution_layers/). Un réseau convolutionnel fonctionne en \"filtrant\" les images à l'aide de différents filtres dont les coefficients sont appris lors de l'étape d'entraînement. Un réseau convolutionnel efficace est typiquement constitué d'une succession de couches convolutives et de pooling (voir par exemple https://www.tensorflow.org/tutorials/images/cnn).\n",
        "\n",
        "Chaque couche convolutive déplace un filtre (dont les coefficients sont fixés lors de l'étape d'entraînement) sur les sorties des couches précédentes\n",
        "\n",
        "\n",
        "<img src=\"same_padding_no_strides.gif\" alt=\"stackoverflow.com\" width=304 height=142>\n",
        "\n",
        "\n",
        "Afin de réduire la dimension des sorties des couches successives, on alterne généralement entre des couches convolutives et des couches de pooling (équivalentes à un sous-échantillonnage) qui retiennent pour une région donnée, uniquement les pixels de plus forte intensité (afin de conserver une trace du contraste). Ces couches sont de la forme suivante:\n",
        "\n",
        "\n",
        "<img src=\"maxPool.png\" alt=\"stackoverflow.com\" width=504 height=142>\n"
      ],
      "metadata": {
        "id": "uiR1fD0Q2Y5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les données MNIST\n",
        "(X, y), _ = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normaliser les données et convertir les labels\n",
        "X = X / 255.0\n",
        "mask = (y == 0) | (y == 1)  # Comparer avec des entiers 0 et 1\n",
        "X, y = X[mask], y[mask].astype(int)\n",
        "\n",
        "# Reshape des données pour les couches convolutives\n",
        "X = X.reshape(-1, 28, 28, 1)\n",
        "y = to_categorical(y, num_classes=2)\n",
        "\n",
        "# Séparer les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Création du modèle convolutionnel\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilation du modèle\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Entraînement du modèle\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Évaluation du modèle\n",
        "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"Taux de classification sur l'ensemble d'entraînement : {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Taux de classification sur l'ensemble de test : {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1ARbegr2SL1",
        "outputId": "b80e39e1-962d-46a6-a9b8-6d8b7c633f4c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.9808 - loss: 0.0616 - val_accuracy: 1.0000 - val_loss: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - accuracy: 0.9990 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 1.9600e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.9995 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 3.7703e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 1.0000 - val_loss: 2.6619e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - accuracy: 0.9998 - loss: 4.5229e-04 - val_accuracy: 1.0000 - val_loss: 8.7945e-06\n",
            "Epoch 6/10\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 31ms/step - accuracy: 0.9998 - loss: 4.1536e-04 - val_accuracy: 1.0000 - val_loss: 3.7860e-05\n",
            "Epoch 7/10\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.9999 - loss: 4.0060e-04 - val_accuracy: 1.0000 - val_loss: 8.7581e-06\n",
            "Epoch 8/10\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 9.3398e-05 - val_accuracy: 1.0000 - val_loss: 6.0627e-07\n",
            "Epoch 9/10\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 5.0908e-06 - val_accuracy: 1.0000 - val_loss: 5.9882e-07\n",
            "Epoch 10/10\n",
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 2.2214e-06 - val_accuracy: 1.0000 - val_loss: 4.3926e-07\n",
            "Taux de classification sur l'ensemble d'entraînement : 100.00%\n",
            "Taux de classification sur l'ensemble de test : 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VrAICQYp4tbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3 (5pts) Astuce du Noyau\n",
        "\n",
        "On souhaite à présent entraîner un modèle de régression linéaire dans l'espace noyau sur les images de 0 et de 1. Afin d'atteindre cet objectif, on procédera comme suit:\n",
        "\n",
        "- Sélectionner un sous-ensemble d'équilibre d'images de 0 et de 1 (on commencera par exemple par prendre 50 images du chiffre 0 et 50 images du chiffre 1).\n",
        "- Compléter la fonction G(x, y) qui renvoie la valeur d'un noyau Gaussien de paramètre sigma, aux points $x$ et $y$.\n",
        "- Compléter la fonction \"optimize\" a l'aide d'une descente de gradient dans l'espace noyau afin qu'elle retourne le vecteur des coefficients $\\lambda_j$ d'un modele du type\n",
        "$$y(x) = \\sum_{j=1}^N \\lambda_j G(x, x_j)$$\n",
        "\n",
        "- Comme pour la question 2, on souhaite implémenter la fonction \"optimize\" de façon à ce qu'elle retourne la liste des taux de classification correcte (sur l' ensemble d'entraînement et de test) pour chacune des itérations.\n"
      ],
      "metadata": {
        "id": "_h5LB9Gq2p91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def G(x, y, sigma):\n",
        "    '''\n",
        "    La fonction doit renvoyer la valeur du noyau gaussien de\n",
        "    variance sigma aux points x et y\n",
        "    '''\n",
        "    diff = x - y\n",
        "    return np.exp(-np.dot(diff, diff) / (2 * sigma**2))\n",
        "\n",
        "def optimize(X, y, sigma, lr=0.01, epochs=100):\n",
        "    '''\n",
        "    La fonction doit renvoyer le vecteur lambda d'un modele de type\n",
        "    y(x) = sum_i lambda_i G(x, x_i, sigma) ou G est le noyeau Gaussien base sur\n",
        "    l'astuce du noyeau\n",
        "    '''\n",
        "    N = X.shape[0]\n",
        "    lbda = np.zeros(N)\n",
        "\n",
        "    def kernel_matrix(X1, X2, sigma):\n",
        "        \"\"\"Calcule la matrice de noyau Gaussien entre deux ensembles.\"\"\"\n",
        "        K = np.zeros((X1.shape[0], X2.shape[0]))\n",
        "        for i in range(X1.shape[0]):\n",
        "            for j in range(X2.shape[0]):\n",
        "                K[i, j] = G(X1[i], X2[j], sigma)\n",
        "        return K\n",
        "\n",
        "    K_train = kernel_matrix(X, X, sigma)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        predictions = np.dot(K_train, lbda)\n",
        "        errors = predictions - y\n",
        "\n",
        "        # Descente de gradient\n",
        "        grad = np.dot(K_train.T, errors) / N\n",
        "        lbda -= lr * grad\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}: Gradient norm = {np.linalg.norm(grad):.4f}\")\n",
        "\n",
        "    # Taux de classification\n",
        "    def predict(X_train, X_test, lbda, sigma):\n",
        "        K_test = kernel_matrix(X_test, X_train, sigma)\n",
        "        return np.sign(np.dot(K_test, lbda))\n",
        "\n",
        "    y_train_pred = predict(X, X, lbda, sigma)\n",
        "    y_test_pred = predict(X, X_test, lbda, sigma)\n",
        "\n",
        "    rate_training = np.mean(y_train_pred == y) * 100\n",
        "    rate_test = np.mean(y_test_pred == y_test) * 100\n",
        "\n",
        "    return lbda, rate_training, rate_test\n",
        "\n",
        "# Charger les données MNIST\n",
        "(X, y), _ = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normaliser les données et convertir les labels\n",
        "X = X / 255.0\n",
        "mask = (y == 0) | (y == 1)  # Comparer avec des entiers 0 et 1\n",
        "X, y = X[mask], y[mask].astype(int)\n",
        "\n",
        "# Aplatir les images 28x28 en vecteurs 784 dimensions\n",
        "X = X.reshape(X.shape[0], -1)\n",
        "\n",
        "# Sélection d'un sous-ensemble équilibré\n",
        "def select_balanced_subset(X, y, n_samples=50):\n",
        "    idx_0 = np.where(y == 0)[0][:n_samples]\n",
        "    idx_1 = np.where(y == 1)[0][:n_samples]\n",
        "    idx = np.hstack([idx_0, idx_1])\n",
        "    np.random.shuffle(idx)\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "X_subset, y_subset = select_balanced_subset(X, y)\n",
        "\n",
        "# Vérifier la taille de X_subset\n",
        "print(f\"Nombre d'échantillons dans X_subset : {X_subset.shape[0]}\")\n",
        "\n",
        "# Séparer les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_subset, y_subset, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entraînement du modèle noyau gaussien\n",
        "sigma = 1.0\n",
        "lbda, rate_training, rate_test = optimize(X_train, y_train, sigma)\n",
        "\n",
        "print(f\"Taux de classification sur l'ensemble d'entraînement : {rate_training:.2f}%\")\n",
        "print(f\"Taux de classification sur l'ensemble de test : {rate_test:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JhaghwZ2pJ-",
        "outputId": "dd9f184a-d4dd-45b0-c17c-f26eda0ccbb5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre d'échantillons dans X_subset : 100\n",
            "Epoch 0: Gradient norm = 0.0804\n",
            "Epoch 10: Gradient norm = 0.0803\n",
            "Epoch 20: Gradient norm = 0.0802\n",
            "Epoch 30: Gradient norm = 0.0801\n",
            "Epoch 40: Gradient norm = 0.0800\n",
            "Epoch 50: Gradient norm = 0.0799\n",
            "Epoch 60: Gradient norm = 0.0797\n",
            "Epoch 70: Gradient norm = 0.0796\n",
            "Epoch 80: Gradient norm = 0.0795\n",
            "Epoch 90: Gradient norm = 0.0794\n",
            "Taux de classification sur l'ensemble d'entraînement : 47.50%\n",
            "Taux de classification sur l'ensemble de test : 60.00%\n"
          ]
        }
      ]
    }
  ]
}