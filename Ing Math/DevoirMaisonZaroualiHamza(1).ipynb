{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTZCXzOasamO"
      },
      "source": [
        "<h1><center>Ingénierie Mathématique 3, Devoir Maison</center></h1>\n",
        "<h1><center>Janvier 2025</center></h1>\n",
        "\n",
        "<h3><center>Total: 15 points<br>\n",
        "    <br>\n",
        "    Début: 1 Janvier 2025<br>\n",
        "    <br>\n",
        "    Fin: 24 Janvier 2025</center></h3>\n",
        "    \n",
        "   \n",
        "<h3><font color='red'>Consignes:</font></h3>\n",
        "\n",
        "<font color='red'>Aucun devoir soumis après le 20 janvier ne sera pris en compte. Les devoirs qui ne respectent pas strictement les consignes reprises sur le site web (http://www.augustincosse.com/wp-content/uploads/2023/03/consignesSoumission2.pdf) ne seront pas pris en compte. En particulier, la soumission consistera en un __unique__ fichier \".ipynb\" envoye par email dans un email dont l'objet sera \"INFO2 Ingenierie Math DM\"\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIuuyTFisamQ"
      },
      "source": [
        "## Question 1 (5pts) Un premier modèle\n",
        "\n",
        "On considère le jeu de données MNIST ci-dessous (les images peuvent être téléchargées via scikit learn ou via keras). On souhaite commencer par entraîner un modèle de régression logistique permettant de différencier les 1 des 0. Pour ce faire on procédera comme suit:\n",
        "\n",
        "- Extraire à partir des données ci-dessous, les images représentant des $1$ ou des $0$.\n",
        "- Séparer les données en un ensemble de test et un ensemble d'entraînement (on gardera 10% des données pour l'ensemble test)\n",
        "- Compléter la fonction \"binary_cross_entropy\" afin que celle-ci retourne la valeur de l'entropie binaire croisée ainsi que le gradient de cette fonction en une image donnée et pour un vecteur de coefficients de régression $\\mathbf{w}$ donne.\n",
        "- Compléter ensuite la fonction “optimisation\" afin qu'elle implémente une descente de gradient sur la fonction d'entropie binaire croisée. On souhaite  renvoyer en sortie le vecteur des coefficients de régression ainsi que (1) le taux de classification (en pourcentage de données correctement classées sur le nombre de données totales) sur les ensembles d'entraînement et de test.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zrQ5wttsamR",
        "outputId": "28e46652-63e1-4607-cd0f-e5b4c693b2cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 0.6931\n",
            "Epoch 10: Loss = 0.1124\n",
            "Epoch 20: Loss = 0.0673\n",
            "Epoch 30: Loss = 0.0501\n",
            "Epoch 40: Loss = 0.0408\n",
            "Epoch 50: Loss = 0.0349\n",
            "Epoch 60: Loss = 0.0308\n",
            "Epoch 70: Loss = 0.0278\n",
            "Epoch 80: Loss = 0.0255\n",
            "Epoch 90: Loss = 0.0236\n",
            "Taux de classification sur l'ensemble d'entraînement : 99.76%\n",
            "Taux de classification sur l'ensemble de test : 99.73%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "\n",
        "# Charger les données MNIST\n",
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
        "\n",
        "# Normaliser les données et convertir les labels\n",
        "X = X / 255.0\n",
        "mask = (y == '0') | (y == '1')\n",
        "X, y = X[mask], y[mask].astype(int)\n",
        "\n",
        "# Fonction sigmoïde\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Fonction binaire croisée et son gradient\n",
        "def binary_cross_entropy(X, y, w):\n",
        "    '''\n",
        "    Calcule la valeur de l'entropie binaire croisée et son gradient.\n",
        "    l(w) = - sum_i t_i*log(sigma(y(x_i))) + (1-t_i)*log(1-sigma(y(x_i)))\n",
        "    ainsi que le gradient de cette fonction.\n",
        "    '''\n",
        "    m = X.shape[0]\n",
        "    z = np.dot(X, w)\n",
        "    predictions = sigmoid(z)\n",
        "\n",
        "    # Calcul de l'entropie binaire croisée\n",
        "    bce_fun = -np.mean(y * np.log(predictions + 1e-8) + (1 - y) * np.log(1 - predictions + 1e-8))\n",
        "\n",
        "    # Calcul du gradient\n",
        "    bce_grad = np.dot(X.T, (predictions - y)) / m\n",
        "\n",
        "    return bce_fun, bce_grad\n",
        "\n",
        "# Fonction d'optimisation\n",
        "def optimisation(w_init, eta, X_train, y_train, X_test, y_test, epochs=100):\n",
        "    '''\n",
        "    Effectue une descente de gradient pour minimiser l'entropie binaire croisée.\n",
        "    Retourne :\n",
        "      - Le vecteur des coefficients optimaux w_opt\n",
        "      - Les taux de classification sur les ensembles d'entraînement et de test\n",
        "    '''\n",
        "    w = w_init\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Calcul de la perte et du gradient\n",
        "        loss, grad = binary_cross_entropy(X_train, y_train, w)\n",
        "\n",
        "        # Mise à jour des coefficients\n",
        "        w -= eta * grad\n",
        "\n",
        "        # Affichage de la perte toutes les 10 itérations\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
        "\n",
        "    # Prédiction\n",
        "    def predict(X, w):\n",
        "        return (sigmoid(np.dot(X, w)) >= 0.5).astype(int)\n",
        "\n",
        "    y_train_pred = predict(X_train, w)\n",
        "    y_test_pred = predict(X_test, w)\n",
        "\n",
        "    # Taux de classification\n",
        "    rate_bce_training = np.mean(y_train_pred == y_train) * 100\n",
        "    rate_bce_test = np.mean(y_test_pred == y_test) * 100\n",
        "\n",
        "    return w, rate_bce_training, rate_bce_test\n",
        "\n",
        "# Initialisation et test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "w_init = np.zeros(X_train.shape[1])\n",
        "eta = 0.1\n",
        "\n",
        "w_opt, rate_bce_training, rate_bce_test = optimisation(w_init, eta, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(f\"Taux de classification sur l'ensemble d'entraînement : {rate_bce_training:.2f}%\")\n",
        "print(f\"Taux de classification sur l'ensemble de test : {rate_bce_test:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqPaDnZTsamS"
      },
      "source": [
        "## Question 2 (5pts) Un petit réseau convolutif\n",
        "\n",
        "On souhaite à présent ameliorer le resultat de la question 1 a l'aide d'un réseau convolutionnel. Pour ce faire, on souhaite utiliser la librairie Keras (voir https://keras.io/) et en particulier, le modele de reseau convolutionnel (https://keras.io/api/layers/convolution_layers/). Un réseau convolutionnel fonctionne en \"filtrant\" les images à l'aide de différents filtres dont les coefficients sont appris lors de l'étape d'entraînement. Un réseau convolutionnel efficace est typiquement constitué d'une succession de couches convolutives et de pooling (voir par exemple https://www.tensorflow.org/tutorials/images/cnn).\n",
        "\n",
        "Chaque couche convolutive déplace un filtre (dont les coefficients sont fixés lors de l'étape d'entraînement) sur les sorties des couches précédentes\n",
        "\n",
        "\n",
        "<img src=\"same_padding_no_strides.gif\" alt=\"stackoverflow.com\" width=304 height=142>\n",
        "\n",
        "\n",
        "Afin de réduire la dimension des sorties des couches successives, on alterne généralement entre des couches convolutives et des couches de pooling (équivalentes à un sous-échantillonnage) qui retiennent pour une région donnée, uniquement les pixels de plus forte intensité (afin de conserver une trace du contraste). Ces couches sont de la forme suivante:\n",
        "\n",
        "\n",
        "<img src=\"maxPool.png\" alt=\"stackoverflow.com\" width=504 height=142>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpltqxE6samS"
      },
      "source": [
        "En utilisant le modele Sequential() de keras et la fonction add() permettant d'ajouter des couches convolutionnelles ou de pooling, ainsi que la fonction compile() permettant d'entrainer le reseau, et en vous aidant de la documentation de tensorFlow (https://www.tensorflow.org/tutorials/images/cnn), entrainer un (petit) reseau convolutionnel sur les images 0 et 1 des donnees MNIST de la question 1 (A nouveau, on retiendra environ 10% de donnees pour l'ensemble test)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW_geIOBsamS",
        "outputId": "95625c8d-13ae-4441-be88-c4dd6d1fa479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 30ms/step - accuracy: 0.9820 - loss: 0.0485 - val_accuracy: 0.9986 - val_loss: 0.0037\n",
            "Epoch 2/10\n",
            "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.9991 - loss: 0.0020 - val_accuracy: 0.9993 - val_loss: 0.0019\n",
            "Epoch 3/10\n",
            "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step - accuracy: 0.9996 - loss: 9.3642e-04 - val_accuracy: 1.0000 - val_loss: 5.2040e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - accuracy: 0.9997 - loss: 6.9381e-04 - val_accuracy: 0.9986 - val_loss: 0.0017\n",
            "Epoch 5/10\n",
            "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - accuracy: 0.9995 - loss: 0.0011 - val_accuracy: 0.9993 - val_loss: 0.0071\n",
            "Epoch 6/10\n",
            "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.9995 - loss: 0.0010 - val_accuracy: 0.9993 - val_loss: 0.0050\n",
            "Epoch 7/10\n",
            "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 28ms/step - accuracy: 0.9997 - loss: 4.4029e-04 - val_accuracy: 1.0000 - val_loss: 3.8245e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.9998 - loss: 3.0112e-04 - val_accuracy: 0.9986 - val_loss: 0.0067\n",
            "Epoch 9/10\n",
            "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 6.0347e-05 - val_accuracy: 1.0000 - val_loss: 1.5558e-05\n",
            "Epoch 10/10\n",
            "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 3.8218e-06 - val_accuracy: 1.0000 - val_loss: 1.8895e-05\n",
            "Taux de classification sur l'ensemble d'entraînement : 100.00%\n",
            "Taux de classification sur l'ensemble de test : 100.00%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Charger les données MNIST\n",
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
        "\n",
        "# Normaliser les données et convertir les labels\n",
        "X = X / 255.0\n",
        "mask = (y == '0') | (y == '1')\n",
        "X, y = X[mask], y[mask].astype(int)\n",
        "\n",
        "# Reshape des données pour les couches convolutives\n",
        "X = X.reshape(-1, 28, 28, 1)\n",
        "y = to_categorical(y, num_classes=2)\n",
        "\n",
        "# Séparer les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Création du modèle convolutionnel\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilation du modèle\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Entraînement du modèle\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Évaluation du modèle\n",
        "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"Taux de classification sur l'ensemble d'entraînement : {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Taux de classification sur l'ensemble de test : {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaAUVZ0MsamS"
      },
      "source": [
        "## Question 3 (5pts) Astuce du Noyau\n",
        "\n",
        "On souhaite à présent entraîner un modèle de régression linéaire dans l'espace noyau sur les images de 0 et de 1. Afin d'atteindre cet objectif, on procédera comme suit:\n",
        "\n",
        "- Sélectionner un sous-ensemble d'équilibre d'images de 0 et de 1 (on commencera par exemple par prendre 50 images du chiffre 0 et 50 images du chiffre 1).\n",
        "- Compléter la fonction G(x, y) qui renvoie la valeur d'un noyau Gaussien de paramètre sigma, aux points $x$ et $y$.\n",
        "- Compléter la fonction \"optimize\" a l'aide d'une descente de gradient dans l'espace noyau afin qu'elle retourne le vecteur des coefficients $\\lambda_j$ d'un modele du type\n",
        "$$y(x) = \\sum_{j=1}^N \\lambda_j G(x, x_j)$$\n",
        "\n",
        "- Comme pour la question 2, on souhaite implémenter la fonction \"optimize\" de façon à ce qu'elle retourne la liste des taux de classification correcte (sur l' ensemble d'entraînement et de test) pour chacune des itérations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItnxEcmXsamT",
        "outputId": "fc8aa96c-7ae2-416c-8702-c08f808ec065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Gradient norm = 0.0846\n",
            "Epoch 10: Gradient norm = 0.0845\n",
            "Epoch 20: Gradient norm = 0.0844\n",
            "Epoch 30: Gradient norm = 0.0843\n",
            "Epoch 40: Gradient norm = 0.0841\n",
            "Epoch 50: Gradient norm = 0.0840\n",
            "Epoch 60: Gradient norm = 0.0839\n",
            "Epoch 70: Gradient norm = 0.0838\n",
            "Epoch 80: Gradient norm = 0.0837\n",
            "Epoch 90: Gradient norm = 0.0836\n",
            "Taux de classification sur l'ensemble d'entraînement : 52.50%\n",
            "Taux de classification sur l'ensemble de test : 40.00%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def G(x, y, sigma):\n",
        "    '''\n",
        "    La fonction doit renvoyer la valeur du noyau gaussien de\n",
        "    variance sigma aux points x et y\n",
        "    '''\n",
        "    diff = x - y\n",
        "    return np.exp(-np.dot(diff, diff) / (2 * sigma**2))\n",
        "\n",
        "def optimize(X, y, sigma, lr=0.01, epochs=100):\n",
        "    '''\n",
        "    La fonction doit renvoyer le vecteur lambda d'un modele de type\n",
        "    y(x) = sum_i lambda_i G(x, x_i, sigma) ou G est le noyeau Gaussien base sur\n",
        "    l'astuce du noyeau\n",
        "    '''\n",
        "    N = X.shape[0]\n",
        "    lbda = np.zeros(N)\n",
        "\n",
        "    def kernel_matrix(X1, X2, sigma):\n",
        "        \"\"\"Calcule la matrice de noyau Gaussien entre deux ensembles.\"\"\"\n",
        "        K = np.zeros((X1.shape[0], X2.shape[0]))\n",
        "        for i in range(X1.shape[0]):\n",
        "            for j in range(X2.shape[0]):\n",
        "                K[i, j] = G(X1[i], X2[j], sigma)\n",
        "        return K\n",
        "\n",
        "    K_train = kernel_matrix(X, X, sigma)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        predictions = np.dot(K_train, lbda)\n",
        "        errors = predictions - y\n",
        "\n",
        "        # Descente de gradient\n",
        "        grad = np.dot(K_train.T, errors) / N\n",
        "        lbda -= lr * grad\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}: Gradient norm = {np.linalg.norm(grad):.4f}\")\n",
        "\n",
        "    # Taux de classification\n",
        "    def predict(X_train, X_test, lbda, sigma):\n",
        "        K_test = kernel_matrix(X_test, X_train, sigma)\n",
        "        return np.sign(np.dot(K_test, lbda))\n",
        "\n",
        "    y_train_pred = predict(X, X, lbda, sigma)\n",
        "    y_test_pred = predict(X, X_test, lbda, sigma)\n",
        "\n",
        "    rate_training = np.mean(y_train_pred == y) * 100\n",
        "    rate_test = np.mean(y_test_pred == y_test) * 100\n",
        "\n",
        "    return lbda, rate_training, rate_test\n",
        "\n",
        "# Charger les données MNIST\n",
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
        "\n",
        "# Normaliser les données et convertir les labels\n",
        "X = X / 255.0\n",
        "mask = (y == '0') | (y == '1')\n",
        "X, y = X[mask], y[mask].astype(int)\n",
        "\n",
        "# Sélection d'un sous-ensemble équilibré\n",
        "def select_balanced_subset(X, y, n_samples=50):\n",
        "    idx_0 = np.where(y == 0)[0][:n_samples]\n",
        "    idx_1 = np.where(y == 1)[0][:n_samples]\n",
        "    idx = np.hstack([idx_0, idx_1])\n",
        "    np.random.shuffle(idx)\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "X_subset, y_subset = select_balanced_subset(X, y)\n",
        "\n",
        "# Séparer les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_subset, y_subset, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entraînement du modèle noyau gaussien\n",
        "sigma = 1.0\n",
        "lbda, rate_training, rate_test = optimize(X_train, y_train, sigma)\n",
        "\n",
        "print(f\"Taux de classification sur l'ensemble d'entraînement : {rate_training:.2f}%\")\n",
        "print(f\"Taux de classification sur l'ensemble de test : {rate_test:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}