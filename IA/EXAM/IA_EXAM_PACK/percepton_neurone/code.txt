import numpy as np
import matplotlib.pyplot as plt
Nd = 100
# droite aléatoire w0 + x_1 * w_1 + x_2 w_2
w = np.random.random(size=(3))*2-1
xr = np.arange(-5,5)
# eq de la séparatrice
yr = -w[0]/w[2] - w[1]/w[2]*xr

# nuage de points
Cloud = np.random.random(size=(2,Nd))*8-4
Marge = np.matmul(Cloud.T,w[1:]) + w[0]
CPos = np.where(Marge > 1)
CNeg = np.where(Marge < -1)
plt.plot(xr,yr)
plt.scatter(Cloud[0,CPos],Cloud[1,CPos])
plt.scatter(Cloud[0,CNeg],Cloud[1,CNeg])
labels = np.zeros(Cloud.shape[1])
labels[np.where(Marge>1)] = 1

CAll = np.where(np.abs(Marge) > 1)
X = Cloud[:,CAll[0]]
y = labels[CAll[0]]

Nt = 50
Xt = np.random.random(size=(2,Nt))*8-4
yt = np.zeros(Xt.shape[1])
Marge_t = np.matmul(Xt.T,w[1:]) + w[0]
yt[np.where(Marge_t>0)] = 1

CPos_t = np.where(Marge_t > 0)
CNeg_t = np.where(Marge_t < 0)

plt.plot(xr,yr)
plt.scatter(X[0,:],X[1,:])
plt.scatter(Xt[0,CPos_t],Xt[1,CPos_t])
plt.scatter(Xt[0,CNeg_t],Xt[1,CNeg_t])
En utilisant la biblothèque ci-dessous, ajustez modèle sur le jeu de données. En particulier vous regarderez

l'évolution de la séparatrice après chaque itération
la courbe score (accuracy)
la courbe de la loss
Pour chacune les deux dernières courbes, vous rajouterez les valeurs obtenues sur l'ensemble test.

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import log_loss
clf = MLPClassifier(solver='sgd', alpha=0, hidden_layer_sizes=(), random_state=5, warm_start=True, activation='logistic', max_iter=1, learning_rate_init=0.002)

TMAX = 200
score_train = np.zeros(TMAX)
score_test = np.zeros(TMAX)
loss_train = np.zeros(TMAX)
loss_test = np.zeros(TMAX)

f,ax = plt.subplots(1,3,figsize=(15,5))


for t in range(TMAX):
    clf.fit(X.T,y)
    yr_clf = -clf.intercepts_[0][0]/clf.coefs_[0][1][0] - clf.coefs_[0][0][0]/clf.coefs_[0][1][0]*xr
    score_train[t] = clf.score(X.T,y)
    score_test[t] = clf.score(Xt.T,yt)
    loss_train[t] = clf.loss_
    loss_test[t] = log_loss(yt,clf.predict_proba(Xt.T))
    ax[2].plot(xr,yr_clf)
    

ax[2].set_ylim([-5,5])
ax[2].plot(xr,yr)
ax[2].scatter(X[0,:],X[1,:])
    
ax[0].plot(score_train)
ax[0].plot(score_test)
ax[1].plot(loss_train)
ax[1].plot(loss_test)
# display predicted scores by the model as a contour plot
from matplotlib.colors import LogNorm

x = np.linspace(-5., 5.)
y = np.linspace(-4.5, 4.5)
X1, Y1 = np.meshgrid(x, y)
XX = np.array([X1.ravel(), Y1.ravel()]).T
Z = clf.predict_proba(XX)[:,0]
Z = Z.reshape(X1.shape)

CS = plt.contour(X1, Y1, Z,
                 levels=np.linspace(0, 1, 100))
CB = plt.colorbar(CS, shrink=1.0, extend='both')

plt.scatter(X[0, :], X[1, :], 10, color='red')
plt.ylim(-5,5)
plt.plot(xr,yr_clf)
